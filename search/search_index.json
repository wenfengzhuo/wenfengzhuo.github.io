{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Archives"},{"location":"2016-11-21-Tree-Traversal/","text":"The traversal of a binary tree is pretty easy to implement and we already know that there are three ways of tree traversal: preorder , inorder , postorder . The traversal with recursion is trivial, so is there any way to travese a tree? This article will discuss different ways to traversal a tree based on my learning. Traverse tree recursively \u00b6 It is pretty straightfoward to traverse a tree using recursion, and also it makes the code simple and easy to understand. The following code snipets are the recursive versions of three ways of tree traversal. Let's define the binary tree structure first: public class TreeNode { public int val ; public TreeNode left ; public TreeNode right ; } Preorder traversal: public void preorder ( TreeNode root , List < Integer > res ) { if ( root == null ) { return ; } res . add ( root . val ); preorder ( root . left , res ); preorder ( root . right , res ); } Inorder traversal: public void inorder ( TreeNode root , List < Integer > res ) { if ( root == null ) { return ; } inorder ( root . left , res ); res . add ( root . val ); inorder ( root . right , res ); } Postorder traversal: public void postorder ( TreeNode root , List < Integer > res ) { if ( root == null ) { return ; } postorder ( root . left , res ); postorder ( root . right , res ); res . add ( root . val ); } From the simple code above, we can clearly conclude that the recursive traversal of a binary tree is just following the natural traversal order. For example, the definition of inorder traversal is that we first visit left subtree, and then current node, and finally the right tree, and the recursive code directly reflects the definition. Traverse tree iteratively \u00b6 How about we traverse a binary tree without recursion? First why are we asking this question. Sometimes, we might be given some very unbalanced tree or even worse the tree is just like a linked list which means every node of the tree only has right child. See the following tree. The direct impact of this input is that we might encounter stackoverflow exception during the program. This is one of the reason that motivates us to use other way to travese a tree. 1 \\ 2 \\ 3 \\ ... How about we traverse a binary tree without recursion? The answer is iterative traversal, although it might be a little bit more complicated than recursion. In my understanding, why it becomes complicated is because we now have to carefully maintain the intermediate status of the traversal. We have to store some nodes in memory because they are not in the right order to be visted, and then we will come back to visit it again. This is probably the reason things become complex. How do we implement the iterative traversal? If we consider this question as a task to convert the recursive traversal to iterative traversal, we will quickly know that we could use stack to accomplish this. Why? Because using a stack is just mimicking the recursion. For a recursion, actually the underline technique is using stack to store the intermediate status of the function. Now the task becomes clear. The task is how to use stack to iteratively traverse tree which is just like traversing a tree recursively. Let's take a look at the inorder traversal of a binary tree iteratively: public List < Integer > inorder ( TreeNode root ) { List < Integer > res = new ArrayList <> (); Stack < TreeNode > stack = new Stack <> (); TreeNode cur = root ; while ( cur != null || ! stack . isEmpty ()) { if ( cur != null ) { stack . push ( cur ); // -> inorder(cur.left, res), with this recursive function call, the cur will be stored in function stack cur = cur . left ; } else { cur = stack . pop (); // -> res.add(cur.val), when inorder(cur.left, res) completes, the cur node will be popped from function stack res . add ( cur . val ); cur = cur . right ; // -> inorder(cur.right, res) } } return res ; } The comments in above code compare the recursive code with the iterative code. We can clearly see that what we actually did is that we maniplated the function call with our own code. With the above understanding, we can easily implement the preroder traversal and inorder traversal. Preorder traversal iteratively: public List < Integer > preorder ( TreeNode root ) { List < Integer > res = new ArrayList <> (); Stack < TreeNode > stack = new Stack <> (); TreeNode cur = root ; while ( cur != null || ! stack . isEmpty ()) { if ( cur != null ) { res . add ( cur . val ); stack . push ( cur ); cur = cur . left ; } else { cur = stack . pop (); cur = cur . right ; } } } Postorder traversal iteratively: public List < Integer > postorder ( TreeNode root ) { List < Integer > res = new ArrayList <> (); Stack < TreeNode > stack = new Stack <> (); TreeNode cur = root ; while ( cur != null || ! stack . isEmpty ()) { if ( cur != null ) { stack . push ( cur ); cur = cur . left ; } else if ( stack . peek (). right != null ) { cur = stack . peek (). right ; stack . push ( cur ); } else { cur = stack . pop (); res . add ( cur . val ); while ( ! stack . isEmpty () && stack . peek (). right == cur ) { cur = stack . pop (); res . add ( cur . val ); } cur = stack . isEmpty () ? null : stack . peek (). right ; } } return res ; } Morris Traversal \u00b6 We are not ending up with using stack to traverse a binary tree. Could we avoid to use stack to traverse the tree? First agian, why are we asked this question? The main reason is that the stack will consume extra memory and another reason could be there is something we didn't use in binary tree which could help us to traverse a tree without a stack. Here we could use the right child link of a leaf node to point to its inorder successor. You may be curious how people come up with this solution, so please refer to Threaded Binary Tree . During the traversal, actually we could store the inorder successor of a leaf node. When we finish the traversal of a leaf node, we could use this link to navigate its successor with O(1) time. See the picture below about how to store the link: . Here is the code of inorder morris traversal: public List < Integer > morrisInorder ( TreeNode root ) { List < Integer > res = new ArrayList <> (); TreeNode cur = root ; while ( cur != null ) { if ( cur . left != null ) { TreeNode pre = cur . left ; while ( pre . right != null && pre . right != cur ) { pre = pre . right ; } if ( pre . right == null ) { pre . right = cur ; // Link to its inorder successor } else { pre . right = null ; // Unlink to make the tree unmodified res . add ( cur . val ); cur = cur . right ; } } else { res . add ( cur . val ); cur = cur . right ; // Right will always exists due to our previous linking } } }","title":"Binary Tree Traversal"},{"location":"2016-11-21-Tree-Traversal/#traverse-tree-recursively","text":"It is pretty straightfoward to traverse a tree using recursion, and also it makes the code simple and easy to understand. The following code snipets are the recursive versions of three ways of tree traversal. Let's define the binary tree structure first: public class TreeNode { public int val ; public TreeNode left ; public TreeNode right ; } Preorder traversal: public void preorder ( TreeNode root , List < Integer > res ) { if ( root == null ) { return ; } res . add ( root . val ); preorder ( root . left , res ); preorder ( root . right , res ); } Inorder traversal: public void inorder ( TreeNode root , List < Integer > res ) { if ( root == null ) { return ; } inorder ( root . left , res ); res . add ( root . val ); inorder ( root . right , res ); } Postorder traversal: public void postorder ( TreeNode root , List < Integer > res ) { if ( root == null ) { return ; } postorder ( root . left , res ); postorder ( root . right , res ); res . add ( root . val ); } From the simple code above, we can clearly conclude that the recursive traversal of a binary tree is just following the natural traversal order. For example, the definition of inorder traversal is that we first visit left subtree, and then current node, and finally the right tree, and the recursive code directly reflects the definition.","title":"Traverse tree recursively"},{"location":"2016-11-21-Tree-Traversal/#traverse-tree-iteratively","text":"How about we traverse a binary tree without recursion? First why are we asking this question. Sometimes, we might be given some very unbalanced tree or even worse the tree is just like a linked list which means every node of the tree only has right child. See the following tree. The direct impact of this input is that we might encounter stackoverflow exception during the program. This is one of the reason that motivates us to use other way to travese a tree. 1 \\ 2 \\ 3 \\ ... How about we traverse a binary tree without recursion? The answer is iterative traversal, although it might be a little bit more complicated than recursion. In my understanding, why it becomes complicated is because we now have to carefully maintain the intermediate status of the traversal. We have to store some nodes in memory because they are not in the right order to be visted, and then we will come back to visit it again. This is probably the reason things become complex. How do we implement the iterative traversal? If we consider this question as a task to convert the recursive traversal to iterative traversal, we will quickly know that we could use stack to accomplish this. Why? Because using a stack is just mimicking the recursion. For a recursion, actually the underline technique is using stack to store the intermediate status of the function. Now the task becomes clear. The task is how to use stack to iteratively traverse tree which is just like traversing a tree recursively. Let's take a look at the inorder traversal of a binary tree iteratively: public List < Integer > inorder ( TreeNode root ) { List < Integer > res = new ArrayList <> (); Stack < TreeNode > stack = new Stack <> (); TreeNode cur = root ; while ( cur != null || ! stack . isEmpty ()) { if ( cur != null ) { stack . push ( cur ); // -> inorder(cur.left, res), with this recursive function call, the cur will be stored in function stack cur = cur . left ; } else { cur = stack . pop (); // -> res.add(cur.val), when inorder(cur.left, res) completes, the cur node will be popped from function stack res . add ( cur . val ); cur = cur . right ; // -> inorder(cur.right, res) } } return res ; } The comments in above code compare the recursive code with the iterative code. We can clearly see that what we actually did is that we maniplated the function call with our own code. With the above understanding, we can easily implement the preroder traversal and inorder traversal. Preorder traversal iteratively: public List < Integer > preorder ( TreeNode root ) { List < Integer > res = new ArrayList <> (); Stack < TreeNode > stack = new Stack <> (); TreeNode cur = root ; while ( cur != null || ! stack . isEmpty ()) { if ( cur != null ) { res . add ( cur . val ); stack . push ( cur ); cur = cur . left ; } else { cur = stack . pop (); cur = cur . right ; } } } Postorder traversal iteratively: public List < Integer > postorder ( TreeNode root ) { List < Integer > res = new ArrayList <> (); Stack < TreeNode > stack = new Stack <> (); TreeNode cur = root ; while ( cur != null || ! stack . isEmpty ()) { if ( cur != null ) { stack . push ( cur ); cur = cur . left ; } else if ( stack . peek (). right != null ) { cur = stack . peek (). right ; stack . push ( cur ); } else { cur = stack . pop (); res . add ( cur . val ); while ( ! stack . isEmpty () && stack . peek (). right == cur ) { cur = stack . pop (); res . add ( cur . val ); } cur = stack . isEmpty () ? null : stack . peek (). right ; } } return res ; }","title":"Traverse tree iteratively"},{"location":"2016-11-21-Tree-Traversal/#morris-traversal","text":"We are not ending up with using stack to traverse a binary tree. Could we avoid to use stack to traverse the tree? First agian, why are we asked this question? The main reason is that the stack will consume extra memory and another reason could be there is something we didn't use in binary tree which could help us to traverse a tree without a stack. Here we could use the right child link of a leaf node to point to its inorder successor. You may be curious how people come up with this solution, so please refer to Threaded Binary Tree . During the traversal, actually we could store the inorder successor of a leaf node. When we finish the traversal of a leaf node, we could use this link to navigate its successor with O(1) time. See the picture below about how to store the link: . Here is the code of inorder morris traversal: public List < Integer > morrisInorder ( TreeNode root ) { List < Integer > res = new ArrayList <> (); TreeNode cur = root ; while ( cur != null ) { if ( cur . left != null ) { TreeNode pre = cur . left ; while ( pre . right != null && pre . right != cur ) { pre = pre . right ; } if ( pre . right == null ) { pre . right = cur ; // Link to its inorder successor } else { pre . right = null ; // Unlink to make the tree unmodified res . add ( cur . val ); cur = cur . right ; } } else { res . add ( cur . val ); cur = cur . right ; // Right will always exists due to our previous linking } } }","title":"Morris Traversal"},{"location":"2016-11-9-what-i-learned-from-building-this-website/","text":"Why choose Jekyll \u00b6 The answer is markdown . There are many articles about why markdown is so popular with developers, so I won't list its advatanges here. Jekyll is both a website generator and also providing a web server that serves the website. It beautifully converts those plain text (markdown or liquid) to human-readable html pages. This is a great feature for developers especially those who are struglling with HTML/CSS stuff. Github Pages vs AWS \u00b6 A blog must be served in a host. Github Pages provides host for repository that either is for github projects or your personal blogs. Actually, Github Pages is powered by Jekyll, so technically, any Jekyll project could work well in Github pages (see -> how to set up a Jekyll project in Github Pages ). Unfortuantely, Github Pages does not support most of beautify Jekyll Themes . Such themes are contributed by thounsand of developers who did a great job at making a beautiful Jekyll pages, and most importantly, they are free to use. This is why I decided to set up AWS . There are many cloud options, so why AWS? First, it is used by countless people; Second, it is probably one of the earliest public cloud infrastrature, so it is pretty mature; Third, it provides starters with a apealling 12-month's free trial. Edit: It turns out I was wrong because we can directly upload whole theme into our git repository. What Github Pages does not support is that it does not allow you to define a theme in _config.xml file. Anyway, using AWS gave me a great opportunity to learn how to use AWS to build website. AWS Network Problem \u00b6 When everything was setup( how? ), I began to navigate the public adress to see whether my blog is ready. Unfortunately, the address cannot be accessed as the Chrome indicated. I was wondering if the public address assigned by AWS is not public yet. This drove me to ping the address in my terminal. It turned out I cannot ping the address either. I dived into the AWS console to see what happened, but the address was a public IP. Then I doubted that is this related to firewall. I denied myself. If it is related to firewall, then at least we could ping the address although we could not access a specific port. Is it related to some configurations? Then I noticed that there is a Network Security Group there in the configuration page, and it turned out be the evil of the issue. With a default setting, we could only ssh to aws instance because only 22 port is open for access, all other inbound and outbound traffic are turned off. See more details -> Security Group Automatically website updating \u00b6 Finally, I want to talk about website updating. Github Pages provides a easy way to display your blog content. What you need to do is committing a change to the repository and the change will be automatically applied. While my blog is served in AWS, so how to automatically update the web content when I want to commit a new post? The first tool I thought of is cron job . Using a cron job to periodically fetch the latest data from github repository, and that's it. The cron job is a very powerful tool in a linux system while it is so easy to configure. Here is my configuration: */1 * * * * <User Name> cd <Blog Path> && git pull >> /var/log/blogcronb.log 2>&1","title":"What I learned from building this blog website"},{"location":"2016-11-9-what-i-learned-from-building-this-website/#why-choose-jekyll","text":"The answer is markdown . There are many articles about why markdown is so popular with developers, so I won't list its advatanges here. Jekyll is both a website generator and also providing a web server that serves the website. It beautifully converts those plain text (markdown or liquid) to human-readable html pages. This is a great feature for developers especially those who are struglling with HTML/CSS stuff.","title":"Why choose Jekyll"},{"location":"2016-11-9-what-i-learned-from-building-this-website/#github-pages-vs-aws","text":"A blog must be served in a host. Github Pages provides host for repository that either is for github projects or your personal blogs. Actually, Github Pages is powered by Jekyll, so technically, any Jekyll project could work well in Github pages (see -> how to set up a Jekyll project in Github Pages ). Unfortuantely, Github Pages does not support most of beautify Jekyll Themes . Such themes are contributed by thounsand of developers who did a great job at making a beautiful Jekyll pages, and most importantly, they are free to use. This is why I decided to set up AWS . There are many cloud options, so why AWS? First, it is used by countless people; Second, it is probably one of the earliest public cloud infrastrature, so it is pretty mature; Third, it provides starters with a apealling 12-month's free trial. Edit: It turns out I was wrong because we can directly upload whole theme into our git repository. What Github Pages does not support is that it does not allow you to define a theme in _config.xml file. Anyway, using AWS gave me a great opportunity to learn how to use AWS to build website.","title":"Github Pages vs AWS"},{"location":"2016-11-9-what-i-learned-from-building-this-website/#aws-network-problem","text":"When everything was setup( how? ), I began to navigate the public adress to see whether my blog is ready. Unfortunately, the address cannot be accessed as the Chrome indicated. I was wondering if the public address assigned by AWS is not public yet. This drove me to ping the address in my terminal. It turned out I cannot ping the address either. I dived into the AWS console to see what happened, but the address was a public IP. Then I doubted that is this related to firewall. I denied myself. If it is related to firewall, then at least we could ping the address although we could not access a specific port. Is it related to some configurations? Then I noticed that there is a Network Security Group there in the configuration page, and it turned out be the evil of the issue. With a default setting, we could only ssh to aws instance because only 22 port is open for access, all other inbound and outbound traffic are turned off. See more details -> Security Group","title":"AWS Network Problem"},{"location":"2016-11-9-what-i-learned-from-building-this-website/#automatically-website-updating","text":"Finally, I want to talk about website updating. Github Pages provides a easy way to display your blog content. What you need to do is committing a change to the repository and the change will be automatically applied. While my blog is served in AWS, so how to automatically update the web content when I want to commit a new post? The first tool I thought of is cron job . Using a cron job to periodically fetch the latest data from github repository, and that's it. The cron job is a very powerful tool in a linux system while it is so easy to configure. Here is my configuration: */1 * * * * <User Name> cd <Blog Path> && git pull >> /var/log/blogcronb.log 2>&1","title":"Automatically website updating"},{"location":"2016-12-12-Gap-Buffer%20Implementation%20in%20Java/","text":"GapList \u00b6 GapList is a Java implementation for Gap Buffer . It allows efficient insertion and deletion operations clustered near the same location. As a result, Gap Buffer is commoly used in text editor. Usage \u00b6 As the implementation implements java.util.List interface, it could be a drop-in replacement of Java built-in java.util.ArrayList. List < Integer > gapList = new GapList < Integer > (); gapList . add ( 1 ); gapList . remove ( 0 ); gapList . addAll ( Arrays . asList ( 0 , 1 , 2 , 3 )); gapList . add ( 0 , 100 ); // achieve high performance if operations are clustered in the same location // A new API is added. removeAll(int fromIndex, int toIndex). It allows client to remove elements // in the list from \"fromIndex\" to \"toIndex\"(exclusive). (( GapList < Integer > ) gapList ). removeAll ( 0 , 3 ); Improve \u00b6 Multiple APIs should be tested Should add range check for safe operation on list A benchmark test is needed to reveal the performance difference between ArrayList and GapList Source \u00b6 Source code is hosted on Github","title":"Gap Buffer implementation in Java"},{"location":"2016-12-12-Gap-Buffer%20Implementation%20in%20Java/#gaplist","text":"GapList is a Java implementation for Gap Buffer . It allows efficient insertion and deletion operations clustered near the same location. As a result, Gap Buffer is commoly used in text editor.","title":"GapList"},{"location":"2016-12-12-Gap-Buffer%20Implementation%20in%20Java/#usage","text":"As the implementation implements java.util.List interface, it could be a drop-in replacement of Java built-in java.util.ArrayList. List < Integer > gapList = new GapList < Integer > (); gapList . add ( 1 ); gapList . remove ( 0 ); gapList . addAll ( Arrays . asList ( 0 , 1 , 2 , 3 )); gapList . add ( 0 , 100 ); // achieve high performance if operations are clustered in the same location // A new API is added. removeAll(int fromIndex, int toIndex). It allows client to remove elements // in the list from \"fromIndex\" to \"toIndex\"(exclusive). (( GapList < Integer > ) gapList ). removeAll ( 0 , 3 );","title":"Usage"},{"location":"2016-12-12-Gap-Buffer%20Implementation%20in%20Java/#improve","text":"Multiple APIs should be tested Should add range check for safe operation on list A benchmark test is needed to reveal the performance difference between ArrayList and GapList","title":"Improve"},{"location":"2016-12-12-Gap-Buffer%20Implementation%20in%20Java/#source","text":"Source code is hosted on Github","title":"Source"},{"location":"2017-03-26-why-I-choose-quantcast/","text":"Disclaimer: The article is based on my personal job searching experience, and every opinion is based on my personal thoughts and has nothing related to other parties or people. Finding a job in US is never an easy task, especially technical jobs though there is a hug demand. I started my job searching in the early September last year, and stopped until I finished my last onsite interview with Quantcast on Feb. 28 this year (2017). During this peroid, I had totally 14 technical phone interviews and I passed 11 of them (Some companies have multiple phone rounds, like Quantcast and SumoLogic). I got 6 onsite interviews after I passed those phone interviews, and finally I received 4 offers from these companies: SumoLogic, Amazon, Uber and Quantcast. Then I realized that signing an offer was even more complicated and strugling. To me, these four companies are great and very attractive. SumoLogic SumoLogic is a healthy and growing startup based in Redwood City, CA. The company provides a cloud log management platform which altenates with the industry leader - Splunk. More and more customers adopt the ideas that collecting and analyzing machine data in cluster is a more cost-efficient way. Despite a startup they claimed, actually they have now nearly 80+ engineers to solve complex and interesting problems. They maintain a large distributed system that process 100+PB data each day. Now they have 1000+ customers and more to come. Amazon Amazon is alreay a well-known giant company. The public company now reaches the market cap at about 400B, which is nearly 50% more than Alibaba. They have three major products that dominates the online retailer market: Marketplace, Prime, and AWS. They call these three products as three pillars in Amazon. My offer from Amazon was not extended by the teams of these three products but rather a inner \"startup\" team - Advertising Team. It is now a team with 800+ engineers and other people, which acts like a fast growing startup in Amazon. I was very excited about this opportunity because I personally believe that advertising will be fourth pillar in Amazon. Uber Uber is an aggressive company and never a startup any more despite that it was just founded at 2009. It is claimed that now Uber is at least worth 60B, which is a huge number that surpasses a lot of big companies. Uber comes from a disruptive idea that aims to solve one of the most complex problem in the world - transportation. Thanks to this huge problem, Uber also grows exponentially along these years and expands now in 550+ cities worldwide. Uber is still continuing disrupting many industries with their existing platform. UberEats is another product that now are widely spread in many cities. I personally believe that more and more disruptive products will be brought by Uber and constantly change and improve people's daily life. Quantcast Quantcast is a profitable company that is not known by lots of people. Being in Ad Tech industry is not a good choice because there are so many companies that compete for pieces of the big \"cake\" - Online Advertising. Quantcast is not just performing and running very well in this industry, but also is one of the few companies who have gained net profit for many years. One of facts could prove their ability to make profit is that their last fund was 2011, 6 years from now. The company provides a powerful free product called Quantcast Measure that is adopted by most of Top 100 websites worldwide, and millions of websites are using the product. This generous product gives them back a huge amount of data that helps Quantcast has precise understanding of users. This understanding further enables Quantcast to perform a lot better than other competitors in prospecting and retargeting. It took me half of month to make a decision, so util March 15th I finally decided where to go. This is my first job in US and it would probably change my life. It was very easy to choose an offer emotionally, but I would like to make a best choice that both follows my heart and at the same time satisfies the following criteria. Company Vision This is the first priority among those criteria. As an engineer, my motivation is always trying to do something that will make a difference to the world. Uber is a company with huge potential to be more successful. Transportation is super large in our modern life, and every car on the road could connect with each other to form a enormous network. In this network, Uber could deliver more things rather than rides and eats. Uber already has their platform that supports thousands of millions requests, and more innovations will happen on their platform. I personally believe that Uber will be super large in the future. Quantcast is also full of potential. Given a fierce competition in ad-tech industry, Quantcast still gains net profit and runs towards the right direction. With the existing ability to make profit and the right leadership, Quantcast will be much more ambitious in the future. Google and Facebook are certainly the largest competitors, but they are more fousing on their own platform: Search and Social. Quantcast could engage in more areas and places. SumoLogic is certainly growing. More and more customers are switching to the cloud-based platform. I believe that the company will grow larger and larger. But personally, I don't see that SumoLogic will be a huge company. They have a very tough competitor - Splunk, and of course a lot of other platforms including open source communities. The need for SumoLogic platform seems not that urgent, which I personally believe that there will be a ceiling for their market. They do make a great impact in log management and machine data analysis, but I don't think it will make a huge difference. Culture Culture is very important to me. I love the culture that favors ownship, responsibility, flexibility, creativity. A great culture will boost employee's performance and help them to deliver great products and features. A great culture also facilitates the good relationships among colleagues, which further improve the efficiency. I alwasy want to join the company that makes me happy to come to office and love Monday. All the four companies have great engineering culture, although only one company that has the culture that concerns me. Despite some bad news reported in Feburary, they actually did not affect me too much about the culture in Uber. There were two things that made me a little bit worried if I would join Uber. During the interview process, I found that people in Uber did not like to reply emails (I don't know this only happens to myself or also to others). When I sent an email to ask for travel accomodations for onsite interview, it took nearly a week to get the reply before the day of my flight. Sometimes a email was sent, it never was replied. This applied to my recruiter, my hiring manager and my coordinator. (I have to say my recruiter and my hiring manager are very nice guys. But I just don't know why they don't like to reply emails). Another thing is their inflexibility. I had 3.5 years working experience and companies like Quantcast, Amazon, SumoLogic treated me just as non-graduate candidate and offered me higher level positions and better compensations than new graduate candidates. Even though I tried all my best to prove that I am not a typical new graduate candiate (my recruiter and my hiring manager were also helping to persuade the committee to give me a more competing offer), unfortunatelly the committee still regarded me as a entry level candidate and refused to provide me a higher level of position. Personal Growth Instead of the company growth, personal growth is also very important. Although the company is great and have a bright future, it is meanless if I join a team in a company where I have few opportunities to grow. Quantcast values me and respected me as a rather senior individual that could contribute more in their team. They offered better compensation and also they assigned more responsibilities to me. Here is the quote from my hiring manager: As for your role on the team, I see you as a rather senior individual. I\u2019d expect you to be taking on projects with a level of ambiguity where you\u2019d be doing full component design. In other words, you\u2019d be a leader and soon enough a mentor in looking at abstract software problems and coming up with compelling designs that you\u2019d socialize with the team. I\u2019d expect you to be advising senior members of the team and raising the bar on good code practice. I believe that there will be more opportunities for me to grow if I join a company that respects and values me. In this sense, I do appreciate that Quantcast could provide me such a great chance to prove myself and exert my influence. I believe that my career goal will have higher possibility to be achieved in Quantcast. This is why I choose Quantcast.","title":"Why I Declined Offers From SumoLogic, Amazon, Uber and Choose Quantcast"},{"location":"2017-1-14-Linked-List/","text":"Implementation \u00b6 Both of Linked List and Array List serve as collections for storing a sequence of elements. The difference between these two lists is their internal implementation of the ways of storing objects. For linked list, every objects stored in it are connected or linked together by maintaining pointers that point its predecessor and successor, so the objects will be represented as a node internally, which holds the information of the object itself, the predecessor pointer and the successor pointer. Adding a new element is as simple as adding a node at the tail of the list. For array list, all objects will be stored in a pre-allocated array. The object will be directly added in the array without wrapping it as a node. The internal array could grow as more objects are added. Performance \u00b6 With knowing their implementation, we could derive their performance clearly from operation perspective. Add For adding new object at the tail of the list, both of linked list and array list will have O(1) time for this operation. A difference is that if all slots in array list are exhausted, the list will grow to hold more objects, then a extra copy from the old array to the new array will be executed. This will cost extra time compared with linked list, although this cost will be amortized to O(1). For adding new object in a position other than the tail of the list, the linked list will cost O(n/2) in worst case because it will first iterate the list to reach the node in the target position and then insert the added object by linking it. For array list, finding the position to insert the added object cost O(1) time, but there is an extra O(n) time in worst case to shift all object (to the right of the target position) to the right by one position. Deletion Linked list will cost O(n/2) time in worst case for delete a object if the target is in the middle of the list. In linked list, the deletion could be achieved by first finding that target node and then unlinking it from its predecessor and successor. Finding the object will cost O(n/2) time, and the unlinking will cost O(1) time. Array list will only cost O(1) time to delete a target object, but it still requires extra time to shift all objects (to the right of the deleted object) to the left. If we delete the first object of the list in array list, then a copy of the rest of objects will be performed and this will nearly cost O(n) time (n is the length of the array). In total, the deletion will cost O(n) time for array list in worst case. Retrieve by index For linked list, retrieving an object by its index in the list will cost O(n/2) time as we need to iterate as most as half of the list to find the object if it resides in the middle of the list. For array list, retrieving an object is as simple as retrieving the object in its internal array, so this will guarantee that it only costs O(1) time. Space Linked list will consume more space than array list as it has overhead to store the pointers, and specifically in Java, there will also be overhead for objects, because linked list will create a node object to represent each data item. Array list directly stores the original data item in an array which makes it consume less space. Application \u00b6 With their differences in implementation and performance, linked list and array list will be used in different scenarios. Random Access If we want to random access to the objects in the list, array list will be a better choice because each retrieval operation will only cost O(1) time. A typical example is that when we want to use quick-sort to sort the objects in the memory, storing those objects in an array list will achieve high performance. Even for sequential access, the array list will generally perform better than linked list thanks to the good locality of reference. A simple explanation of locality of reference is that the memory near the already accessed memory is tended to be cached so retrieving it will be faster than retrieving those in RAM. Density of Data If the data that is stored in a grid-way, or there are multiple dimensions to access the data, and the data is sparse distributed, then it is better to use linked list. A typical example is sparse matrix. Using array list would consume too much unnecessary memory in this case. Another example I could think up is the representation of a polynomial equation. Manipulating data If we want to insert or delete data during the iterator of the list, linked list will outperform than array list. This is because array list will cost extra time to move data when adding or deleting an element in a specific. access order A common requirement in practice is that we might need to maintain the access order of the objects, for example a LRU cache. Using linked list to implement this cache is an easy task. The recently accessed object could be put in the tail of the list, and the front object of the list will be the one which is least recently used, and if the cache reaches its capacity, we could evict the front objects. Both of those operations will cost only O(1) time. In Java, it is very quick to implement a LRU cache by inheriting LinkedHashMap and overriding its removeEldestEntry method. memory As the overhead of memory to store objects in linked list, it is always better to choose array list to store large amount of data in memory. Basic data structure Linked list usually is the choice to be used as the underlying data structures of queues and stacks thanks to its O(1) operations of adding and removing object at the head or tail of the list.","title":"Linked List vs Array List"},{"location":"2017-1-14-Linked-List/#implementation","text":"Both of Linked List and Array List serve as collections for storing a sequence of elements. The difference between these two lists is their internal implementation of the ways of storing objects. For linked list, every objects stored in it are connected or linked together by maintaining pointers that point its predecessor and successor, so the objects will be represented as a node internally, which holds the information of the object itself, the predecessor pointer and the successor pointer. Adding a new element is as simple as adding a node at the tail of the list. For array list, all objects will be stored in a pre-allocated array. The object will be directly added in the array without wrapping it as a node. The internal array could grow as more objects are added.","title":"Implementation"},{"location":"2017-1-14-Linked-List/#performance","text":"With knowing their implementation, we could derive their performance clearly from operation perspective. Add For adding new object at the tail of the list, both of linked list and array list will have O(1) time for this operation. A difference is that if all slots in array list are exhausted, the list will grow to hold more objects, then a extra copy from the old array to the new array will be executed. This will cost extra time compared with linked list, although this cost will be amortized to O(1). For adding new object in a position other than the tail of the list, the linked list will cost O(n/2) in worst case because it will first iterate the list to reach the node in the target position and then insert the added object by linking it. For array list, finding the position to insert the added object cost O(1) time, but there is an extra O(n) time in worst case to shift all object (to the right of the target position) to the right by one position. Deletion Linked list will cost O(n/2) time in worst case for delete a object if the target is in the middle of the list. In linked list, the deletion could be achieved by first finding that target node and then unlinking it from its predecessor and successor. Finding the object will cost O(n/2) time, and the unlinking will cost O(1) time. Array list will only cost O(1) time to delete a target object, but it still requires extra time to shift all objects (to the right of the deleted object) to the left. If we delete the first object of the list in array list, then a copy of the rest of objects will be performed and this will nearly cost O(n) time (n is the length of the array). In total, the deletion will cost O(n) time for array list in worst case. Retrieve by index For linked list, retrieving an object by its index in the list will cost O(n/2) time as we need to iterate as most as half of the list to find the object if it resides in the middle of the list. For array list, retrieving an object is as simple as retrieving the object in its internal array, so this will guarantee that it only costs O(1) time. Space Linked list will consume more space than array list as it has overhead to store the pointers, and specifically in Java, there will also be overhead for objects, because linked list will create a node object to represent each data item. Array list directly stores the original data item in an array which makes it consume less space.","title":"Performance"},{"location":"2017-1-14-Linked-List/#application","text":"With their differences in implementation and performance, linked list and array list will be used in different scenarios. Random Access If we want to random access to the objects in the list, array list will be a better choice because each retrieval operation will only cost O(1) time. A typical example is that when we want to use quick-sort to sort the objects in the memory, storing those objects in an array list will achieve high performance. Even for sequential access, the array list will generally perform better than linked list thanks to the good locality of reference. A simple explanation of locality of reference is that the memory near the already accessed memory is tended to be cached so retrieving it will be faster than retrieving those in RAM. Density of Data If the data that is stored in a grid-way, or there are multiple dimensions to access the data, and the data is sparse distributed, then it is better to use linked list. A typical example is sparse matrix. Using array list would consume too much unnecessary memory in this case. Another example I could think up is the representation of a polynomial equation. Manipulating data If we want to insert or delete data during the iterator of the list, linked list will outperform than array list. This is because array list will cost extra time to move data when adding or deleting an element in a specific. access order A common requirement in practice is that we might need to maintain the access order of the objects, for example a LRU cache. Using linked list to implement this cache is an easy task. The recently accessed object could be put in the tail of the list, and the front object of the list will be the one which is least recently used, and if the cache reaches its capacity, we could evict the front objects. Both of those operations will cost only O(1) time. In Java, it is very quick to implement a LRU cache by inheriting LinkedHashMap and overriding its removeEldestEntry method. memory As the overhead of memory to store objects in linked list, it is always better to choose array list to store large amount of data in memory. Basic data structure Linked list usually is the choice to be used as the underlying data structures of queues and stacks thanks to its O(1) operations of adding and removing object at the head or tail of the list.","title":"Application"},{"location":"2017-1-14-Union-Iterators/","text":"Input : M iterators N tuples in each iterator K keys in each tuple Output : The ordered union of all tuples from each iterators. Assumption : 1 Tuples are different to each other in the same iterator 2 Each iterator has exact the same number of tuples The question above could be simplified as: merge M iterators into a single stream and keep those tuples sorted in the final stream . As tuples in a single iterator are already in order, what we need to take care is to maintain the relative order among the different iterators during the merging process. With this objective, I have two solutions in mind. Priority Queue \u00b6 Algorithm Why does a priority queue help? The thinking process is pretty straightforward. As we want to make the final single stream as sorted, during the merging process if we always pick the smallest elements from those M iterators and put it to our final stream, then we will finally get a sorted single stream. To pick the smallest elements efficiently, priority queue (or heap) might be the most appropriate choice. We could maintain a min-heap, which has the smallest element in the root. The retrieval of smallest element and the addition of a element to the queue will both cost O(log(n)) time, n is the number of elements in the queue. Pseudo Code The following pseudo code elaborates the algorithm above. To make it simple, the code assumes that the final stream r (a list) will have a add() function, the function will append an element in this tail and also it will ignore those elements who are the same with the tail so that the final stream does not contain duplicates. For the priority queue, we will assume that it could compare the first element of a iterator. union(Iterator[] A): r = [] q = priority queue for i in A if i has next q.offer(i) ====> (1) O(Klog(M)) * M while q has next i = q.poll() ====> (2) O(Klog(M) * N * M r.add(i.next()) if i has next q.offer(i) ====> (3) O(Klog(M)) * (N*M - M) return r Complexity In the code above, we could see that we first add those iterators who have elements into the priority queue. Then we will retrieve the smallest element every time we poll the root from the priority queue. After we add the smallest element in our final stream, we also need to make sure that we will add the iterator back if it still has elements in it. In line (1), we can see the program will execute M times if we have M iterators, for each addition into the queue, it will be O(log(M)) comparisons as of the property of priority queue. We have to notice that, for each comparison, it will cost O(K) time to compare a tuple since there are K keys in each tuple. So in total, the line (1) will contribute O(Klog(M)) * M to the time complexity. In line (3), we know that there are still (N*M - M) elements that are needed to be offered in the queue, so in total, it will be O(Klog(M))*(N*M - M). All in all, the total time complexity of this algorithm will be O(N * K * M * log(M)) in worst case. To be more precise, as the number of comparisons matters if K is relatively not small, we could dive deeper to see more precise number of comparisons for this algorithm. From the code above, we could know that each tuple in those iterators will be offered in the queue and polled out of the queue for once separately. As a result, there will be 2 * N * M operations with the queue. For poll operation, by knowing the property of heap, we know that it will end up with 2 * log(M) comparisons because it requires two comparisons to determine which key is smaller during the repairing of the heap. For offer operation, it will require another log(M). In total, there will be 3 * N * M * log(M) comparisons, and hence the total time complexity will be O(3 * N * K * M * log(M)). As for the space complexity, we know that during the merging process, there will always be M elements if all iterators still have remaining elements. So in worst case, it will be O(M) elements in queue, and each elements has O(K) keys, so in total, the space complexity will be O(M * K). In conclusion: Time Complexity: O(N * K * M * log(M)) Space Complexity: O(M * K) Divide-and-Conquer \u00b6 Algorithm Another way to think this problem is to find whether there is an approach to simplify the problem or convert the problem to an existing solved problem. We could start with a very simple case: if there are only two iterators to be merged. In this case, the situation is just like a the merging phase in merge-sort, so we could use the same logic to solve the problem in this case. If there are more iterators, we could always divide those iterators into halves, and solve it recursively until there are only two iterators. This logic is pretty much the same with merge-sort. Pseudo Code mergeIterator is a function that merges two sorted iterators into a single iterable object. The function assumes that the iterator has a peek function that returns the first elements in the iterator. mergeIterator(Iterator l, Iterator r) r = [] while l has next && r has next kl = l.peek() kr = r.peek() if kl <= kr ====> O(K) r.add(l.next()) else r.add(r.next()) while l has next r.add(l.next()) while r has next r.add(r.next()) return r union is the major function in this algorithm. It adopts the same idea with merge-sort. It divides the iterator array into halves recursively, and then use mergeIterator function to merge the iterators that are returned by the two recursive call. union(Iterator[] A) len = len of A if len == 0 return [] if len == 1 return [](A[0]) mid = len / 2 l = union(A[:mid]) ====> (1) f(M/2) r = union(A[mid+1:]) ====> (2) f(M/2) return mergeIterator(l, r) ====> (3) O(N * M * K) => O(N * k * M) Complexity As we can see in the code, the time complexity follows the formula of a typical divide-and-conquer solution: T(n) = 2*T(n/2) + f(n) In the line (3), we can see it calls mergeIterator function. The mergeIterator function has time complexity proportional to the size of the two iterators, because there will be the same number of comparisons with the number of elements in these two iterators. As a result, line (3) indicates that f(n) = O(N*M*K)=O(NK * M). According to the master theorem, the final time complexity of this algorithm will be O(N * K * M * log(M)). Let's see the number of comparisons in this algorithm. In mergeIterator function, the maximum number of comparison is equal to the sum of the number of elements in the two given iterators (for example, if the two iterators are [1, 3, 5], [2, 4, 5], the number of comparison will be 3 + 3 - 1). The number of comparisons follows the divide-and-conquer formula: C(n) = 2 * C(n/2) + f(n) The f(n) = N * M, when n = M, then the exact maximum number of comparison will be O(N * M * log(M)), hence the total time complexity will be O(N * K * M * log(M)). For space complexity, as you can see from the code, there will be no extra space needed if we do not count the stack space for recursive call. With the space for recursive call considered, the space complexity will be O(log(M)). In conclusion: Time Complexity: O(N * K * M * log (M)) Space Complexity: O(log(M)) Comparison between queue and divide-and-conquer \u00b6 Time Complexity From the time complexity perspective, both of the two functions will have the same time complexity, which means that theoretically, both of the two algorithms will have nearly same running time for the same input. While taking the constant factor into consideration, the divide-and-conquer has smaller constant factor due to the fewer comparison during the merging process. In general, the divide-and-conquer solution performs better than the queue solution. Space Complexity As we could tell clear from the descriptions above, the divide-and-conquer solution will requires much less space than the priority queue solution. In practice, if we adopt divide-and-conquer solution, it means we only need few extra memory to achieve the same time complexity with the priority queue solution. In fact, in practice more extra memory might have impact on the total running time. When request more memory, new memory will be allocated, and this process will cost some time. What is more, if the data is too large to fit in memory, then virtual memory will be involved, which has significantly lower performance than the RAM memory. In this perspective, I will prefer divide-and-conquer solution. Parallel capability By using divide-and-conquer method, we know that the second approach could parallelize very well. To simply put, we could run the two halves of the iterator arrays at the same time, and this applies to every recursive call. By using parallelism, in assumption that we have enough processors, the time complexity will be reduced to O(N*M*K). Further discussion will be covered in next section. For priority queue solution, since we need to get the smallest tuples every time we poll from the queue, it is hard for us to parallelize the algorithm. Further Optimization \u00b6 Parallelism As we already mentioned in previous section, we could parallelize divide-and-conquer solution in a very natural way. The following pseudo code explains how could achieve parallelism: union(Iterator[] A) len = len of A if len == 0 return [] if len == 1 return [](A[0]) mid = len / 2 fork l = union(A[:mid]) ====> (1) r = union(A[mid+1:]) join ====> (2) return mergeIterator(l, r) In line (1), we create a new thread (or process) to run the recursive call for the left half of the array in parallel. In line(2), we will wait the completion of the two processes. So the time complexity could be calculated with the formula: T(M) = T(M/2) + O(M * N * K) As a result, the final time complexity will be O(N * K * M). Tuple Comparison For a tuple, assign a hash value for it. The hash value is determined by the keys in each tuple. The large tuple will have large hash value. Then for most of keys, just compare their hash value, we will know their order. For the tuples who have the same hash value, we compare them by comparing the keys in each tuple. The following picture shows a way of hashing a tuple. Let's assume the range of keys in those tuples is 0~1023, and there are 3 keys in each tuples. We divide the whole range with 2^3 = 8 parts, each part will be labeled as 0, 1, 2, ..., 7. The following code explains how we compute the hash for a tuple: t (k1, k2, k3). hash(tuple) h = 0 i = 0 while i < 3 if tuple[i] >= 512 h |= 1 << (2-i) i ++ return h Let's assume each tuple now has a function hashCode which returns a pre-computed hash. If luckily, all the tuples in those iterators have different hash value, then we could directly reduce the time complexity of our algorithm to O(N*M*log(M)). In reality, some tuples might have the same hash value. So we need to do an amortized analysis of the algorithm. If all the tuples distributed normally in the picture above, then we could calculate the how many tuples might end up with having the same hash values. For two tuples, the possibility to have the same hash value will be 1 / (2 ^ k). We know that there will N*M comparisons in total, so for those comparisons which need full comparisons of each keys, the total number will be (1/(2^k)) * (N*M). Let's assume there is c percentage of comparison which could directly use hash values. So the average time for each comparison will be ((1/(2^k)*(N*M)) * K + cN*M)/(M*N) = K/(2^K) + c As c is lower than 1 (because not all comparisons could directly use hash values), so the average time for each comparison will less than 2. The time complexity now will be from O(N*K*M*log(M)) to O(N*M*log(M)\uff09 because the average time for comparison becomes constant time. Combination Theoretically, if we combine above two optimizations, we could achieve O(N * M) time Benchmark \u00b6 As to demonstrate the algorithms described above and its performance, I implemented them with Java. The source code could be found here: Union Iterators The following pictures show the results from the benchmark tests for those two algorithms. Picture 1 : The running time for different number of iterations to be merged. (M changed) Picture 2 : The running time for different number of elements in each iterator. (N changed). For the picture 1 , we could see that when number of iterators to be merged grows, divide-and-conquer solution will perform better than priority queue solution. This is what we expected from the analysis of the algorithm. Although there are some difference in running time, the two solutions differ only in a small scale, which might indicates the constant factor might not cause huge performance gap between these two solutions. For the picture 2 , an interesting thing to note that is the priority queue solution outperforms than the divide-and-conquer solution when number of elements in each iterator grows. This could be interpreted as the the M is small so it will not gain too much from the benefits of divide-and-conquer solutions, and the function call during the recursive process might cost some overhead, which make it slower than the priority queue solution. From the two pictures above, we could conclude that, in real-world practice, when M is usually large, we might choose divide-and-conquer solution, and when N is usually larger than M, we could choose priority queue solution. In the two pictures, we could see that there is a third line called ParallelMerger . It is the parallelized version of divide-and-conquer solution. Unfortunately, it does not meet our expectation. One explanation could be that there is thread-switching overhead that makes it slower than non-parallelized algorithm. Or maybe I could implement in a different way for ParallelMerger .","title":"Union Iterators"},{"location":"2017-1-14-Union-Iterators/#priority-queue","text":"Algorithm Why does a priority queue help? The thinking process is pretty straightforward. As we want to make the final single stream as sorted, during the merging process if we always pick the smallest elements from those M iterators and put it to our final stream, then we will finally get a sorted single stream. To pick the smallest elements efficiently, priority queue (or heap) might be the most appropriate choice. We could maintain a min-heap, which has the smallest element in the root. The retrieval of smallest element and the addition of a element to the queue will both cost O(log(n)) time, n is the number of elements in the queue. Pseudo Code The following pseudo code elaborates the algorithm above. To make it simple, the code assumes that the final stream r (a list) will have a add() function, the function will append an element in this tail and also it will ignore those elements who are the same with the tail so that the final stream does not contain duplicates. For the priority queue, we will assume that it could compare the first element of a iterator. union(Iterator[] A): r = [] q = priority queue for i in A if i has next q.offer(i) ====> (1) O(Klog(M)) * M while q has next i = q.poll() ====> (2) O(Klog(M) * N * M r.add(i.next()) if i has next q.offer(i) ====> (3) O(Klog(M)) * (N*M - M) return r Complexity In the code above, we could see that we first add those iterators who have elements into the priority queue. Then we will retrieve the smallest element every time we poll the root from the priority queue. After we add the smallest element in our final stream, we also need to make sure that we will add the iterator back if it still has elements in it. In line (1), we can see the program will execute M times if we have M iterators, for each addition into the queue, it will be O(log(M)) comparisons as of the property of priority queue. We have to notice that, for each comparison, it will cost O(K) time to compare a tuple since there are K keys in each tuple. So in total, the line (1) will contribute O(Klog(M)) * M to the time complexity. In line (3), we know that there are still (N*M - M) elements that are needed to be offered in the queue, so in total, it will be O(Klog(M))*(N*M - M). All in all, the total time complexity of this algorithm will be O(N * K * M * log(M)) in worst case. To be more precise, as the number of comparisons matters if K is relatively not small, we could dive deeper to see more precise number of comparisons for this algorithm. From the code above, we could know that each tuple in those iterators will be offered in the queue and polled out of the queue for once separately. As a result, there will be 2 * N * M operations with the queue. For poll operation, by knowing the property of heap, we know that it will end up with 2 * log(M) comparisons because it requires two comparisons to determine which key is smaller during the repairing of the heap. For offer operation, it will require another log(M). In total, there will be 3 * N * M * log(M) comparisons, and hence the total time complexity will be O(3 * N * K * M * log(M)). As for the space complexity, we know that during the merging process, there will always be M elements if all iterators still have remaining elements. So in worst case, it will be O(M) elements in queue, and each elements has O(K) keys, so in total, the space complexity will be O(M * K). In conclusion: Time Complexity: O(N * K * M * log(M)) Space Complexity: O(M * K)","title":"Priority Queue"},{"location":"2017-1-14-Union-Iterators/#divide-and-conquer","text":"Algorithm Another way to think this problem is to find whether there is an approach to simplify the problem or convert the problem to an existing solved problem. We could start with a very simple case: if there are only two iterators to be merged. In this case, the situation is just like a the merging phase in merge-sort, so we could use the same logic to solve the problem in this case. If there are more iterators, we could always divide those iterators into halves, and solve it recursively until there are only two iterators. This logic is pretty much the same with merge-sort. Pseudo Code mergeIterator is a function that merges two sorted iterators into a single iterable object. The function assumes that the iterator has a peek function that returns the first elements in the iterator. mergeIterator(Iterator l, Iterator r) r = [] while l has next && r has next kl = l.peek() kr = r.peek() if kl <= kr ====> O(K) r.add(l.next()) else r.add(r.next()) while l has next r.add(l.next()) while r has next r.add(r.next()) return r union is the major function in this algorithm. It adopts the same idea with merge-sort. It divides the iterator array into halves recursively, and then use mergeIterator function to merge the iterators that are returned by the two recursive call. union(Iterator[] A) len = len of A if len == 0 return [] if len == 1 return [](A[0]) mid = len / 2 l = union(A[:mid]) ====> (1) f(M/2) r = union(A[mid+1:]) ====> (2) f(M/2) return mergeIterator(l, r) ====> (3) O(N * M * K) => O(N * k * M) Complexity As we can see in the code, the time complexity follows the formula of a typical divide-and-conquer solution: T(n) = 2*T(n/2) + f(n) In the line (3), we can see it calls mergeIterator function. The mergeIterator function has time complexity proportional to the size of the two iterators, because there will be the same number of comparisons with the number of elements in these two iterators. As a result, line (3) indicates that f(n) = O(N*M*K)=O(NK * M). According to the master theorem, the final time complexity of this algorithm will be O(N * K * M * log(M)). Let's see the number of comparisons in this algorithm. In mergeIterator function, the maximum number of comparison is equal to the sum of the number of elements in the two given iterators (for example, if the two iterators are [1, 3, 5], [2, 4, 5], the number of comparison will be 3 + 3 - 1). The number of comparisons follows the divide-and-conquer formula: C(n) = 2 * C(n/2) + f(n) The f(n) = N * M, when n = M, then the exact maximum number of comparison will be O(N * M * log(M)), hence the total time complexity will be O(N * K * M * log(M)). For space complexity, as you can see from the code, there will be no extra space needed if we do not count the stack space for recursive call. With the space for recursive call considered, the space complexity will be O(log(M)). In conclusion: Time Complexity: O(N * K * M * log (M)) Space Complexity: O(log(M))","title":"Divide-and-Conquer"},{"location":"2017-1-14-Union-Iterators/#comparison-between-queue-and-divide-and-conquer","text":"Time Complexity From the time complexity perspective, both of the two functions will have the same time complexity, which means that theoretically, both of the two algorithms will have nearly same running time for the same input. While taking the constant factor into consideration, the divide-and-conquer has smaller constant factor due to the fewer comparison during the merging process. In general, the divide-and-conquer solution performs better than the queue solution. Space Complexity As we could tell clear from the descriptions above, the divide-and-conquer solution will requires much less space than the priority queue solution. In practice, if we adopt divide-and-conquer solution, it means we only need few extra memory to achieve the same time complexity with the priority queue solution. In fact, in practice more extra memory might have impact on the total running time. When request more memory, new memory will be allocated, and this process will cost some time. What is more, if the data is too large to fit in memory, then virtual memory will be involved, which has significantly lower performance than the RAM memory. In this perspective, I will prefer divide-and-conquer solution. Parallel capability By using divide-and-conquer method, we know that the second approach could parallelize very well. To simply put, we could run the two halves of the iterator arrays at the same time, and this applies to every recursive call. By using parallelism, in assumption that we have enough processors, the time complexity will be reduced to O(N*M*K). Further discussion will be covered in next section. For priority queue solution, since we need to get the smallest tuples every time we poll from the queue, it is hard for us to parallelize the algorithm.","title":"Comparison between queue and divide-and-conquer"},{"location":"2017-1-14-Union-Iterators/#further-optimization","text":"Parallelism As we already mentioned in previous section, we could parallelize divide-and-conquer solution in a very natural way. The following pseudo code explains how could achieve parallelism: union(Iterator[] A) len = len of A if len == 0 return [] if len == 1 return [](A[0]) mid = len / 2 fork l = union(A[:mid]) ====> (1) r = union(A[mid+1:]) join ====> (2) return mergeIterator(l, r) In line (1), we create a new thread (or process) to run the recursive call for the left half of the array in parallel. In line(2), we will wait the completion of the two processes. So the time complexity could be calculated with the formula: T(M) = T(M/2) + O(M * N * K) As a result, the final time complexity will be O(N * K * M). Tuple Comparison For a tuple, assign a hash value for it. The hash value is determined by the keys in each tuple. The large tuple will have large hash value. Then for most of keys, just compare their hash value, we will know their order. For the tuples who have the same hash value, we compare them by comparing the keys in each tuple. The following picture shows a way of hashing a tuple. Let's assume the range of keys in those tuples is 0~1023, and there are 3 keys in each tuples. We divide the whole range with 2^3 = 8 parts, each part will be labeled as 0, 1, 2, ..., 7. The following code explains how we compute the hash for a tuple: t (k1, k2, k3). hash(tuple) h = 0 i = 0 while i < 3 if tuple[i] >= 512 h |= 1 << (2-i) i ++ return h Let's assume each tuple now has a function hashCode which returns a pre-computed hash. If luckily, all the tuples in those iterators have different hash value, then we could directly reduce the time complexity of our algorithm to O(N*M*log(M)). In reality, some tuples might have the same hash value. So we need to do an amortized analysis of the algorithm. If all the tuples distributed normally in the picture above, then we could calculate the how many tuples might end up with having the same hash values. For two tuples, the possibility to have the same hash value will be 1 / (2 ^ k). We know that there will N*M comparisons in total, so for those comparisons which need full comparisons of each keys, the total number will be (1/(2^k)) * (N*M). Let's assume there is c percentage of comparison which could directly use hash values. So the average time for each comparison will be ((1/(2^k)*(N*M)) * K + cN*M)/(M*N) = K/(2^K) + c As c is lower than 1 (because not all comparisons could directly use hash values), so the average time for each comparison will less than 2. The time complexity now will be from O(N*K*M*log(M)) to O(N*M*log(M)\uff09 because the average time for comparison becomes constant time. Combination Theoretically, if we combine above two optimizations, we could achieve O(N * M) time","title":"Further Optimization"},{"location":"2017-1-14-Union-Iterators/#benchmark","text":"As to demonstrate the algorithms described above and its performance, I implemented them with Java. The source code could be found here: Union Iterators The following pictures show the results from the benchmark tests for those two algorithms. Picture 1 : The running time for different number of iterations to be merged. (M changed) Picture 2 : The running time for different number of elements in each iterator. (N changed). For the picture 1 , we could see that when number of iterators to be merged grows, divide-and-conquer solution will perform better than priority queue solution. This is what we expected from the analysis of the algorithm. Although there are some difference in running time, the two solutions differ only in a small scale, which might indicates the constant factor might not cause huge performance gap between these two solutions. For the picture 2 , an interesting thing to note that is the priority queue solution outperforms than the divide-and-conquer solution when number of elements in each iterator grows. This could be interpreted as the the M is small so it will not gain too much from the benefits of divide-and-conquer solutions, and the function call during the recursive process might cost some overhead, which make it slower than the priority queue solution. From the two pictures above, we could conclude that, in real-world practice, when M is usually large, we might choose divide-and-conquer solution, and when N is usually larger than M, we could choose priority queue solution. In the two pictures, we could see that there is a third line called ParallelMerger . It is the parallelized version of divide-and-conquer solution. Unfortunately, it does not meet our expectation. One explanation could be that there is thread-switching overhead that makes it slower than non-parallelized algorithm. Or maybe I could implement in a different way for ParallelMerger .","title":"Benchmark"},{"location":"2017-1-4-Anycalc/","text":"Anycalc is an extensible library I developed recently, which aims to let developers easily create calculators for caclulating on customized objects rather than just numbers. To create a new calculator on customzied objects, the library requires developers to define their own operands and operators, and the rest of things (evaluate the expressions etc.) will be taken care of by the library. Usage \u00b6 Let's define a new calculator that operates on strings. Before we use the library to implement the calculator, we have to define some use cases for this brand-new calculator. concatenation: given two strings, the calculator could concatenate the two strings. remove: given two string s1, s2, the calculator could remove all s2 appeared in s1. repeated: given a string s1 and a number n, the calculator could repeat s1 by n times. ... (we could define more) First Step With the library, we need to define our operand. Since string is supported by java, there is no need to create a new object. All we need to do is use it as follows: Operand<String> operand = new Operand<String>() Second Step Now we need to define our operators. With the use cases above, we could define the operators as follows: \"+\": concatenate two strings s1, s2. The expression will be: s1 + s2 = s1s2 \"-\": given s1, s2, remove all s2 appeared in s1. The expression will be s1 - s2 = s3 (s3 will not contain any s2 substring) \"*\": given s1, n, repeated s1 by n times. The expresssion will be s1 * n = s1s1...s1 (repeat s1 n times) Let's take the repeated case as example, the code will looks like the follows: public class StringMultiplyOperator implements Operator<String> { @Override public Operand<String> apply(List<Operand<String>> operands) { assert operands.size() == 2; String str = operands.get(0).getValue(); int repeatedNum = Integer.parseInt(operands.get(1).getValue()); StringBuilder sb = new StringBuilder(); while (repeatedNum -- > 0) { sb.append(str); } return new Operand<>(sb.toString()); } @Override public int priority() { return 2; } @Override public int numOfOperands() { return 2; } @Override public String getRep() { return \"*\"; } } For any operators, it should implement the four functions that is required by the library. The four functions are: - apply(): define how the operator operates on the given operands - priority(): define the priority of the operator. This is similiar to number calculator. \"*\" has higher priority than \"+\". - numberOfOperands(): define how many operands the operator requires. 2 means the operators is binary, so 1 means the operator is unary. - getRep(): define the representation of each operators. Third Step Most of things are done now (actually, for current implementation, the library requires a operator factory and a customized calculator instance. Later on, I will remove this part to make it more easier). Let's see a real example of how to use our newly created calculators. How it works \u00b6 The underline principles for the library is pretty easy and straightforward. There is no fancy algorithms or third-party tools that are used by Anycalc. The library adopts the popular Reverse Polish Notation {:target=\"_blank\"}, a common methodology used in computer science, to evaluate the expressions. In the library, the workflow could be represented by the following diagram: To make the library extensible and robust, the library leverages on some design patterns that are widely used in software development, for example, Factory Design Pattern {:target=\"_blank\"}. I made a diagram to show the relationship of different objects in the library. (note: Double Calculator is what we daily used calculator that works on numbers) Future work \u00b6 There are still many things that should be improved in this library. The todo items for this library are: - Dynamic scanner to automatically load new operators and operands - Add more powerful tokenization logic for given expressions - More unit tests to make the code robust Source code \u00b6 Anycalc {:target=\"_blank\"}","title":"Anycalc - make everything calculable"},{"location":"2017-1-4-Anycalc/#usage","text":"Let's define a new calculator that operates on strings. Before we use the library to implement the calculator, we have to define some use cases for this brand-new calculator. concatenation: given two strings, the calculator could concatenate the two strings. remove: given two string s1, s2, the calculator could remove all s2 appeared in s1. repeated: given a string s1 and a number n, the calculator could repeat s1 by n times. ... (we could define more) First Step With the library, we need to define our operand. Since string is supported by java, there is no need to create a new object. All we need to do is use it as follows: Operand<String> operand = new Operand<String>() Second Step Now we need to define our operators. With the use cases above, we could define the operators as follows: \"+\": concatenate two strings s1, s2. The expression will be: s1 + s2 = s1s2 \"-\": given s1, s2, remove all s2 appeared in s1. The expression will be s1 - s2 = s3 (s3 will not contain any s2 substring) \"*\": given s1, n, repeated s1 by n times. The expresssion will be s1 * n = s1s1...s1 (repeat s1 n times) Let's take the repeated case as example, the code will looks like the follows: public class StringMultiplyOperator implements Operator<String> { @Override public Operand<String> apply(List<Operand<String>> operands) { assert operands.size() == 2; String str = operands.get(0).getValue(); int repeatedNum = Integer.parseInt(operands.get(1).getValue()); StringBuilder sb = new StringBuilder(); while (repeatedNum -- > 0) { sb.append(str); } return new Operand<>(sb.toString()); } @Override public int priority() { return 2; } @Override public int numOfOperands() { return 2; } @Override public String getRep() { return \"*\"; } } For any operators, it should implement the four functions that is required by the library. The four functions are: - apply(): define how the operator operates on the given operands - priority(): define the priority of the operator. This is similiar to number calculator. \"*\" has higher priority than \"+\". - numberOfOperands(): define how many operands the operator requires. 2 means the operators is binary, so 1 means the operator is unary. - getRep(): define the representation of each operators. Third Step Most of things are done now (actually, for current implementation, the library requires a operator factory and a customized calculator instance. Later on, I will remove this part to make it more easier). Let's see a real example of how to use our newly created calculators.","title":"Usage"},{"location":"2017-1-4-Anycalc/#how-it-works","text":"The underline principles for the library is pretty easy and straightforward. There is no fancy algorithms or third-party tools that are used by Anycalc. The library adopts the popular Reverse Polish Notation {:target=\"_blank\"}, a common methodology used in computer science, to evaluate the expressions. In the library, the workflow could be represented by the following diagram: To make the library extensible and robust, the library leverages on some design patterns that are widely used in software development, for example, Factory Design Pattern {:target=\"_blank\"}. I made a diagram to show the relationship of different objects in the library. (note: Double Calculator is what we daily used calculator that works on numbers)","title":"How it works"},{"location":"2017-1-4-Anycalc/#future-work","text":"There are still many things that should be improved in this library. The todo items for this library are: - Dynamic scanner to automatically load new operators and operands - Add more powerful tokenization logic for given expressions - More unit tests to make the code robust","title":"Future work"},{"location":"2017-1-4-Anycalc/#source-code","text":"Anycalc {:target=\"_blank\"}","title":"Source code"},{"location":"2017-2-10-Implement-A%20Brand-new%20CQL%20Query%20Statement%20in%20Apache%20Cassandra/","text":"Introduction \u00b6 This patch implemented a new type of query that supports query of max timestamp of all rows under a partition key. The following picture shows how to use this new type of query: The picture shows four scenarios for this query. query the max timestamp for a partition key (the key is single) query the max timestamp for a partition key (the key is composite) query the max timestamp for a non-exist key (will return a empty result) query the max timestamp with insufficient number of values for partition key (will raise an error) The solution created a new grammar in CQL and an independent statement. It uses built-in ReadCommand to read all rows, so it follows the same path with SelectStatement. The difference is that it does not require any columns to be returned, which could help to reduce much I/O operations. As the data is offered online, so every row will be read at least once, hence the time complexity for this query will be O(n) (n represents the number of rows in this partition key). As much less data is returned, it will not cause much consumption of memory. Hence, both time complexity and memory complexity will satisfy the requirements. As for the shortage of this implementation, it uses built-in ReadCommand to read the data, although all the data we need is the metadata of those rows. As the max timestamp is for a partition key, we could maintain a \"max timestamp\" metadata item for each partition key. When inserting, updating or deleting a row under this partition key, we directly update its \"max timestamp\". In this way, we could retrieve the max timestamp in O(1) time. But for this solution, it requires extra efforts to make sure the max timestamp is updated correctly due to concurrency, which might cause performance issue. If this is a used heavily by users, then we could implement a more efficient solution otherwise the former solution will be preferred as above described trade-offs. Pull Request \u00b6 Github PR {:target=\"_blank\"}","title":"Implement a new brand-new CQL query statement in Apache Cassandra"},{"location":"2017-2-10-Implement-A%20Brand-new%20CQL%20Query%20Statement%20in%20Apache%20Cassandra/#introduction","text":"This patch implemented a new type of query that supports query of max timestamp of all rows under a partition key. The following picture shows how to use this new type of query: The picture shows four scenarios for this query. query the max timestamp for a partition key (the key is single) query the max timestamp for a partition key (the key is composite) query the max timestamp for a non-exist key (will return a empty result) query the max timestamp with insufficient number of values for partition key (will raise an error) The solution created a new grammar in CQL and an independent statement. It uses built-in ReadCommand to read all rows, so it follows the same path with SelectStatement. The difference is that it does not require any columns to be returned, which could help to reduce much I/O operations. As the data is offered online, so every row will be read at least once, hence the time complexity for this query will be O(n) (n represents the number of rows in this partition key). As much less data is returned, it will not cause much consumption of memory. Hence, both time complexity and memory complexity will satisfy the requirements. As for the shortage of this implementation, it uses built-in ReadCommand to read the data, although all the data we need is the metadata of those rows. As the max timestamp is for a partition key, we could maintain a \"max timestamp\" metadata item for each partition key. When inserting, updating or deleting a row under this partition key, we directly update its \"max timestamp\". In this way, we could retrieve the max timestamp in O(1) time. But for this solution, it requires extra efforts to make sure the max timestamp is updated correctly due to concurrency, which might cause performance issue. If this is a used heavily by users, then we could implement a more efficient solution otherwise the former solution will be preferred as above described trade-offs.","title":"Introduction"},{"location":"2017-2-10-Implement-A%20Brand-new%20CQL%20Query%20Statement%20in%20Apache%20Cassandra/#pull-request","text":"Github PR {:target=\"_blank\"}","title":"Pull Request"},{"location":"2020-03-14-System%20Design%20-%201/","text":"The article is still in draft stage. System Design is an essential skill for every software engineer. The magnitude of involvement will differ from person to person based on their experience and seniority. System design is also a critically important part in the interview evaluation process. It shows how well you understand software system and computer science applied to real-life problems. While abundant resource about how to be an expert on system design, it still remains as a difficult skill for engineers to obtain. Usually, people are biased by years of experience when judging your system design capability. More extremely, some people might not believe you are able to design a complicated system without years of experience above a threshold. It\u2019s indeed intricate but not without possibility for us to excel it even with few practical experience. If we can derive some common patterns in designing a robust system, we should be able to leverage them to come up with novel solutions. We usually see in many well-design systems, there are \u201ctrade-offs\u201d all over the place. Trade-off is when you trade A with B to maximize your win. The key here is to determine whether it\u2019s A or B you want to trade. It will be a function of many factors: the problem you are solving, the resource you have, the timeline, the cost of solutions, the users the system is serving, the community, the culture of your company, etc.. These factors ultimately make system design a difficult problem to tackle. Essentially in my point of view, it\u2019s a \u201ctrade-off science\u201d. Another key element in this \u201ctrade-off science\u201d is the ability to identify the A and the B in your system that you must choose between. Usually it comes with the pattern: if I choose A, B will suffer; or if I select B, A will be compromised. A and B can be very specific to the system you are designing, but there exists a quite number of common A and B in most of systems. In fact, those specific to your system might be just derivates of the generic factors. For example, in a distributed system, we have often heard of the terms: availability, consistency, latency. They are examples of the A and B. With these two areas to focus, we can attempt to generalize the approaches to system design. We can start off with a discussion about the common factors of a system design and then some common ways to make the trade-offs among those factors. In the following series of articles, I will attempt to cover several topics about system design and some interesting examples. Hopefully it will shed some light on a systematic system design approach for you.","title":"System Design Part 1 - Trade-off Science"},{"location":"2020-04-01-System%20Design%20-%202/","text":"The article is still in draft stage. Before we dig into various aspects of system designs, we shall first review and retrospect where we start when asked for designing a system. High-Level Architecture \u00b6 We can start with a high-level architecture, similar to Figure-1 below. Before long, you will find that nearly majority of systems you are about to design follow the same pattern. A client sends a request to a server, and then the server queries a database. The database sends back some data or confirmation and the server optionally processes the data before sending a response to the client. This diagram seems specific to an online system, for example a website, but can be applied to many problems if you think of the client , server , database in a more general way. client can be: a real user, a web browser, a programming language library, a console or terminal, a batch job driver server can be: any server that accepts requests from clients and responds accordingly. For example, a web server, a load balancer, a DNS server, a cache system, a batch job master server database can be: any system that persists any data for future usage Examples of Systems \u00b6 Let\u2019s review some examples. URL Shortening System \u00b6 In a URL shortening system (commonly asked in a technical interview), a client can be a website similar to bitly.com , which interacts with users who wants to shorten a long link in his hand. Once a user enters his link in the text box and click the Shorten button, a http request will be sent to a remote server with the link to be shortened and optionally the user\u2019s information if she/he logins in. In a small system, the server might just be a single machine that listens a TCP/IP port and exchanges data with outside world. The server processes the request, namely shortening the provided link by using some common techniques like hashing, and store relevant data in a database that can be relational database like MySQL, PostgresSQL or non-relational database such as MongoDB, Cassandra, etc. In a large system that serves millions of users, the server is actually a distributed system that probably comprise of load balancer, tons of commodity machines running the same web service. Additionally, a caching layer will be placed in an appropriate position (we shall revisit more details later about caching system). There will be clusters of database instances which are either leader(s) or followers (a side note: I will avoid using master/slave term as much as possible, but they are the same concepts as leader/follower.). Summary of URL Shortening System Client : a real user or websites or Javascript that interacts with servers Server : a web server listens on 80 or 433 port, or a cluster of servers and load balancers Database : MySQL or PostgresSQL, or MongoDB, DynamoDB, Cassandra In-memory Caching System \u00b6 Caching layer is commonly found in many software systems. A famous example is memcached - an open source key-value based in-memory caching system. In such a system, there might exist a variety of types of clients. Libraries for major programming languages will be available (such as here for Java, here for Python, and here for PHP). In fact, you can even use telnet to interact with memcached server. On the server side, usually it uses a hash table in memory and the table size can grow to as much as the memory capacity. A special characteristic about caching system is that it usually doesn\u2019t have a persistent layer. The primary reason is that caching system is not source of truth for the data. Summary of In-memory Caching System Client : Java/Python/PHP library or telnet Server : a server with a big hash table or a cluster of servers Database : usually no persistent layer A Batch Processing System \u00b6 Batch processing system is quite different from the above two in the first glance. Common batch processing includes Hadoop and Spark . While paradigm of designing is a bit different, we can still manage to use the abstract architecture to describe them. In such a system, a driver program can be regarded as a client. It initiates a batch processing job and submits it to the cluster of the system. The system comprises of master nodes and worker nodes. Master nodes respond to clients\u2019 requests and coordinate jobs with worker nodes, which execute tasks independently. This cluster usually relies on distributed file system to persist data where we can refer as the \u201cdatabase\u201d for the batch process system. HDFS is an open sourced distributed file system commonly used in the industry. Summary of Batching Processing System Client : a driver program Server : the cluster of the system which has master nodes and worker nodes Database : a distributed file system. How to Differentiate \u00b6 Surprisingly, the above three systems are quite different but can all be described with a single formula: client - server - database. If it applies to all systems, what is the point of system design? There must be some unique aspects about each particular system so that the design is essential in building and maintaining it. Systematically, two types of requirements make a system unique regardless of how it is similar to any other system you encountered: Functional Requirements Non-Functional Requirements Functional Requirements These requirements describe the functionalities the system provide. In the URL shortening system, the requirements are: shorten a given URL and store it, retrieve original URL given a shortened URL; In the distributed caching system, the requirements are: store a piece of data associated with a key, retrieve the data with given key. When designing these systems, you have to make sure the system will correctly satisfy these requirements. Correct implementation is the key, which requires you to: * Understand the problem you are solving deeply * Know the use cases of the system * Take care of corner cases * Leverage known algorithms and data structures or devise your own Often, we easily overlook the importance of the functional requirements and focus too much on the non-functional requirements. Consequently, we need to rework the design later or deviate from the initial goal of creating the system. Non-functional Requirements These requirements describe how well the system will serve its functionalities. Usually, it is to ensure: user experience, data integrity, cost effectiveness, low maintenance effort, etc. With these goals in mind, you system needs to take into consideration of a subset of the following things: The availability of the system (it should be the same for a single user or a billion of users) The latency of responding client requests The consistency of the data (whether it is consistent for two different users and/or at two different times and/or in two different geolocations) The cost (machines, development, maintenance) Different systems have different requirements, which make them differentiated from others. In URL shortening system, you need to keep it up running 7/24 and ensure generating the shortened URL quickly enough to make user believe it is worth their effort in using the tool rather than retrieving the long URL directly from their notebook. In the distributed caching system, the system should guarantee a high level of consistency of the data it caches. When a user updates the value of a key, another user should see the change of the same key after a short period of time. There are many techniques for satisfying these non-functional requirements, and we shall discuss them in details separately. Summary \u00b6 When tasked with a system design, it's helpful if we can systematically approach it. The general suggestion here is to: Start with exploring the functional requirements of the system Build a high-level architecture of the system Scope the non-functional requirements and examine the trade-off of different factors","title":"System Design Part 2 - How we approach"},{"location":"2020-04-01-System%20Design%20-%202/#high-level-architecture","text":"We can start with a high-level architecture, similar to Figure-1 below. Before long, you will find that nearly majority of systems you are about to design follow the same pattern. A client sends a request to a server, and then the server queries a database. The database sends back some data or confirmation and the server optionally processes the data before sending a response to the client. This diagram seems specific to an online system, for example a website, but can be applied to many problems if you think of the client , server , database in a more general way. client can be: a real user, a web browser, a programming language library, a console or terminal, a batch job driver server can be: any server that accepts requests from clients and responds accordingly. For example, a web server, a load balancer, a DNS server, a cache system, a batch job master server database can be: any system that persists any data for future usage","title":"High-Level Architecture"},{"location":"2020-04-01-System%20Design%20-%202/#examples-of-systems","text":"Let\u2019s review some examples.","title":"Examples of Systems"},{"location":"2020-04-01-System%20Design%20-%202/#url-shortening-system","text":"In a URL shortening system (commonly asked in a technical interview), a client can be a website similar to bitly.com , which interacts with users who wants to shorten a long link in his hand. Once a user enters his link in the text box and click the Shorten button, a http request will be sent to a remote server with the link to be shortened and optionally the user\u2019s information if she/he logins in. In a small system, the server might just be a single machine that listens a TCP/IP port and exchanges data with outside world. The server processes the request, namely shortening the provided link by using some common techniques like hashing, and store relevant data in a database that can be relational database like MySQL, PostgresSQL or non-relational database such as MongoDB, Cassandra, etc. In a large system that serves millions of users, the server is actually a distributed system that probably comprise of load balancer, tons of commodity machines running the same web service. Additionally, a caching layer will be placed in an appropriate position (we shall revisit more details later about caching system). There will be clusters of database instances which are either leader(s) or followers (a side note: I will avoid using master/slave term as much as possible, but they are the same concepts as leader/follower.). Summary of URL Shortening System Client : a real user or websites or Javascript that interacts with servers Server : a web server listens on 80 or 433 port, or a cluster of servers and load balancers Database : MySQL or PostgresSQL, or MongoDB, DynamoDB, Cassandra","title":"URL Shortening System"},{"location":"2020-04-01-System%20Design%20-%202/#in-memory-caching-system","text":"Caching layer is commonly found in many software systems. A famous example is memcached - an open source key-value based in-memory caching system. In such a system, there might exist a variety of types of clients. Libraries for major programming languages will be available (such as here for Java, here for Python, and here for PHP). In fact, you can even use telnet to interact with memcached server. On the server side, usually it uses a hash table in memory and the table size can grow to as much as the memory capacity. A special characteristic about caching system is that it usually doesn\u2019t have a persistent layer. The primary reason is that caching system is not source of truth for the data. Summary of In-memory Caching System Client : Java/Python/PHP library or telnet Server : a server with a big hash table or a cluster of servers Database : usually no persistent layer","title":"In-memory Caching System"},{"location":"2020-04-01-System%20Design%20-%202/#a-batch-processing-system","text":"Batch processing system is quite different from the above two in the first glance. Common batch processing includes Hadoop and Spark . While paradigm of designing is a bit different, we can still manage to use the abstract architecture to describe them. In such a system, a driver program can be regarded as a client. It initiates a batch processing job and submits it to the cluster of the system. The system comprises of master nodes and worker nodes. Master nodes respond to clients\u2019 requests and coordinate jobs with worker nodes, which execute tasks independently. This cluster usually relies on distributed file system to persist data where we can refer as the \u201cdatabase\u201d for the batch process system. HDFS is an open sourced distributed file system commonly used in the industry. Summary of Batching Processing System Client : a driver program Server : the cluster of the system which has master nodes and worker nodes Database : a distributed file system.","title":"A Batch Processing System"},{"location":"2020-04-01-System%20Design%20-%202/#how-to-differentiate","text":"Surprisingly, the above three systems are quite different but can all be described with a single formula: client - server - database. If it applies to all systems, what is the point of system design? There must be some unique aspects about each particular system so that the design is essential in building and maintaining it. Systematically, two types of requirements make a system unique regardless of how it is similar to any other system you encountered: Functional Requirements Non-Functional Requirements Functional Requirements These requirements describe the functionalities the system provide. In the URL shortening system, the requirements are: shorten a given URL and store it, retrieve original URL given a shortened URL; In the distributed caching system, the requirements are: store a piece of data associated with a key, retrieve the data with given key. When designing these systems, you have to make sure the system will correctly satisfy these requirements. Correct implementation is the key, which requires you to: * Understand the problem you are solving deeply * Know the use cases of the system * Take care of corner cases * Leverage known algorithms and data structures or devise your own Often, we easily overlook the importance of the functional requirements and focus too much on the non-functional requirements. Consequently, we need to rework the design later or deviate from the initial goal of creating the system. Non-functional Requirements These requirements describe how well the system will serve its functionalities. Usually, it is to ensure: user experience, data integrity, cost effectiveness, low maintenance effort, etc. With these goals in mind, you system needs to take into consideration of a subset of the following things: The availability of the system (it should be the same for a single user or a billion of users) The latency of responding client requests The consistency of the data (whether it is consistent for two different users and/or at two different times and/or in two different geolocations) The cost (machines, development, maintenance) Different systems have different requirements, which make them differentiated from others. In URL shortening system, you need to keep it up running 7/24 and ensure generating the shortened URL quickly enough to make user believe it is worth their effort in using the tool rather than retrieving the long URL directly from their notebook. In the distributed caching system, the system should guarantee a high level of consistency of the data it caches. When a user updates the value of a key, another user should see the change of the same key after a short period of time. There are many techniques for satisfying these non-functional requirements, and we shall discuss them in details separately.","title":"How to Differentiate"},{"location":"2020-04-01-System%20Design%20-%202/#summary","text":"When tasked with a system design, it's helpful if we can systematically approach it. The general suggestion here is to: Start with exploring the functional requirements of the system Build a high-level architecture of the system Scope the non-functional requirements and examine the trade-off of different factors","title":"Summary"},{"location":"2020-04-09-ml-system-papers/","text":"Machine learning system is a specialized system that is built for solving problems with machine learning deeply integrated. While it has existed for a while, machine learning is just becoming popular in recent decades compared with other technologies. As more machine learning based solutions are deployed in production, the system around ML has evolved as well. Majority of ML systems are tackling specific problems such as personalization, recommendation and ads ranking. There is also a trend in generalized machine learning system that solves a variety of problems. To learn how a machine learning system is built, the best way is probably through practice with real-life projects. The second best way is by reading papers published by well-known companies that specialize in applied machine learning. Below is a list of good papers that I personally have learned a lot from, and hopefully you will distill useful knowledge for your interests. TFX: A TensorFlow-Based Production-Scale Machine Learning ABSTRACT Creating and maintaining a platform for reliably producing and deploying machine learning models requires careful or- chestration of many components\u2014a learner for generating models based on training data, modules for analyzing and val- idating both data as well as models, and finally infrastructure for serving models in production. This becomes particularly challenging when data changes over time and fresh models need to be produced continuously. Unfortunately, such or- chestration is often done ad hoc using glue code and custom scripts developed by individual teams for specific use cases, leading to duplicated effort and fragile systems with high technical debt. Practical Lessons from Predicting Clicks on Ads at Facebook ABSTRACT Online advertising allows advertisers to only bid and pay for measurable user responses, such as clicks on ads. As a consequence, click prediction systems are central to most on- line advertising systems. With over 750 million daily active users and over 1 million active advertisers, predicting clicks on Facebook ads is a challenging machine learning task. In this paper we introduce a model which combines decision trees with logistic regression, outperforming either of these methods on its own by over 3%, an improvement with sig- nificant impact to the overall system performance. We then explore how a number of fundamental parameters impact the final prediction performance of our system. Not surpris- ingly, the most important thing is to have the right features: those capturing historical information about the user or ad dominate other types of features. Once we have the right features and the right model (decisions trees plus logistic re- gression), other factors play small roles (though even small improvements are important at scale). Picking the optimal handling for data freshness, learning rate schema and data sampling improve the model slightly, though much less than adding a high-value feature, or picking the right model to begin with. Predictive model performance: offline and online evaluations ABSTRACT We study the accuracy of evaluation metrics used to estimate the efficacy of predictive models. Offline evaluation metrics are indicators of the expected model performance on real data. However, in practice we often experience substantial discrepancy between the offline and online performance of the models. We investigate the characteristics and behaviors of the evaluation metrics on offline and online testing both analytically and empirically by experimenting them on online advertising data from the Bing search engine. One of our findings is that some offline metrics like AUC (the Area Under the Receiver Operating Characteristic Curve) and RIG (Relative Information Gain) that summarize the model performance on the entire spectrum of operating points could be quite misleading sometimes and result in significant discrepancy in offline and online metrics. For example, for click prediction models for search advertising, errors in predictions in the very low range of predicted click scores impact the online performance much more negatively than errors in other regions. Most of the offline metrics we studied including AUC and RIG, however, are insensitive to such model behavior. We designed a new model evaluation paradigm that simulates the online behavior of predictive models. For a set of ads selected by a new prediction model, the online user behavior is estimated from the historic user behavior in the search logs. The experimental results on click prediction model for search advertising are highly promising. Wide & Deep Learning for Recommender Systems ABSTRACT Generalized linear models with nonlinear feature transfor- mations are widely used for large-scale regression and clas- sification problems with sparse inputs. Memorization of fea- ture interactions through a wide set of cross-product feature transformations are effective and interpretable, while gener- alization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize bet- ter to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item inter- actions are sparse and high-rank. In this paper, we present Wide & Deep learning\u2014jointly trained wide linear models and deep neural networks\u2014to combine the benefits of mem- orization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisi- tions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow. Deep Neural Networks for YouTube Recommendations ABSTRACT YouTube represents one of the largest scale and most sophis- ticated industrial recommendation systems in existence. In this paper, we describe the system at a high level and fo- cus on the dramatic performance improvements brought by deep learning. The paper is split according to the classic two-stage information retrieval dichotomy: first, we detail a deep candidate generation model and then describe a sepa- rate deep ranking model. We also provide practical lessons and insights derived from designing, iterating and maintain- ing a massive recommendation system with enormous user- facing impact. Factorization Machines ABSTRACT In this paper, we introduce Factorization Machines (FM) which are a new model class that combines the advantages of Support Vector Machines (SVM) with factorization models. Like SVMs, FMs are a general predictor working with any real valued feature vector. In contrast to SVMs, FMs model all interactions between variables using factorized parameters. Thus they are able to estimate interactions even in problems with huge sparsity (like recommender systems) where SVMs fail. We show that the model equation of FMs can be calculated in linear time and thus FMs can be optimized directly. So unlike nonlinear SVMs, a transformation in the dual form is not necessary and the model parameters can be estimated directly without the need of any support vector in the solution. We show the relationship to SVMs and the advantages of FMs for parameter estimation in sparse settings. On the other hand there are many different factorization mod- els like matrix factorization, parallel factor analysis or specialized models like SVD++, PITF or FPMC. The drawback of these models is that they are not applicable for general prediction tasks but work only with special input data. Furthermore their model equations and optimization algorithms are derived individually for each task. We show that FMs can mimic these models just by specifying the input data (i.e. the feature vectors). This makes FMs easily applicable even for users without expert knowledge in factorization models. SlowFast Networks for Video Recognition ABSTRACT We present SlowFast networks for video recognition. Our model involves (i) a Slow pathway, operating at low frame rate, to capture spatial semantics, and (ii) a Fast path- way, operating at high frame rate, to capture motion at fine temporal resolution. The Fast pathway can be made very lightweight by reducing its channel capacity, yet can learn useful temporal information for video recognition. Our models achieve strong performance for both action classification and detection in video, and large improve- ments are pin-pointed as contributions by our SlowFast con- cept. We report state-of-the-art accuracy on major video recognition benchmarks, Kinetics, Charades and AVA. Code has been made available at: https://github.com/facebookresearch/SlowFast. Searching for Communities: a Facebook Way ABSTRACT Giving people the power to build community is central to Face- book\u2019s mission. Technically, searching for communities poses very different challenges compared to the standard IR problems. First, there is a vocabulary mismatch problem since most of the content of the communities is private. Second, the common labeling strategies based on human ratings and clicks do not work well due to limited public content available to third-party raters and users at search time. Finally, community search has a dual objective of satisfying searchers and growing the number of active communities. While A/B testing is a well known approach for assessing the former, it is an open question on how to measure progress on the latter. This talk discusses these challenges in depth and describes our solution. Some paper-like articles Rules of Machine Learning: Best Practices for ML Engineering Introduction This document is intended to help those with a basic knowledge of machine learning get the benefit of best practices in machine learning from around Google. It presents a style for machine learning, similar to the Google C++ Style Guide and other popular guides to practical programming. If you have taken a class in machine learning, or built or worked on a machine\u00adlearned model, then you have the necessary background to read this document. Meet Michelangelo: Uber\u2019s Machine Learning Platform Introduction Uber Engineering is committed to developing technologies that create seamless, impactful experiences for our customers. We are increasingly investing in artificial intelligence (AI) and machine learning (ML) to fulfill this vision. At Uber, our contribution to this space is Michelangelo, an internal ML-as-a-service platform that democratizes machine learning and makes scaling AI to meet the needs of business as easy as requesting a ride. Introducing FBLearner Flow: Facebook\u2019s AI backbone Introduction We decided to build a brand-new platform, FBLearner Flow, capable of easily reusing algorithms in different products, scaling to run thousands of simultaneous custom experiments, and managing experiments with ease. This platform provides innovative functionality, like automatic generation of UI experiences from pipeline definitions and automatic parallelization of Python code using futures. FBLearner Flow is used by more than 25 percent of Facebook's engineering team. Since its inception, more than a million models have been trained, and our prediction service has grown to make more than 6 million predictions per second. Note: Please use the link of the article to share the content rather than any form of copy-paste.","title":"ML System Papers You Should Read"},{"location":"Build%20Your%20Own%20Search%20Engine%20-%201/","text":"","title":"Build Your Own Search Engine - 1"}]}